{"text": "Title: DNABERT: pre-trained Bidirectional Encoder Representations from Transformers for DNA sequence analysis\n\nAuthors: Zhang, L, Wang, J, Chen, X, Li, Y\n\nJournal: Nature Methods\n\nPublication Date: 2023-06-15\n\nBenchmarks: ENCODE, 1000 Genomes, TCGA\n\nKeywords: transformer, deep learning, neural network, machine learning, BERT, attention, foundation model, pre-trained", "pmid": "12345678", "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers for DNA sequence analysis", "score": 122.0, "tier": "gold_standard", "benchmarks": ["ENCODE", "1000 Genomes", "TCGA"]}
{"text": "Title: Enformer: Predicting gene expression from DNA sequence using attention mechanisms\n\nAuthors: Avsec, Z, Agarwal, V, Visentin, D, Ledsam, JR\n\nJournal: Nature\n\nPublication Date: 2023-09-20\n\nBenchmarks: ENCODE\n\nKeywords: deep learning, neural network, attention", "pmid": "12345679", "title": "Enformer: Predicting gene expression from DNA sequence using attention mechanisms", "score": 108.0, "tier": "gold_standard", "benchmarks": ["ENCODE"]}
{"text": "Title: Multi-modal deep learning for cancer genomics and transcriptomics\n\nAuthors: Garcia, P, Martinez, L, Rodriguez, A, Lopez, M\n\nJournal: Nature Communications\n\nPublication Date: 2023-11-05\n\nBenchmarks: TCGA, GTEx\n\nKeywords: transformer, deep learning", "pmid": "12345682", "title": "Multi-modal deep learning for cancer genomics and transcriptomics", "score": 110.0, "tier": "gold_standard", "benchmarks": ["TCGA", "GTEx"]}
