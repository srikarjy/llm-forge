# ğŸ§¬ ScientificLLM-Forge

**A Complete MLOps Platform for Scientific Language Model Development & Deployment**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](https://github.com/scientificllmforge/scientific-llm-forge)

## ğŸš€ Overview

ScientificLLM-Forge is a **production-ready MLOps platform** specifically designed for fine-tuning and deploying Large Language Models on scientific datasets. It provides a complete **"papers â†’ training â†’ serving"** workflow with memory-efficient QLoRA fine-tuning, genomics-specific processing, and production-grade inference serving.

### ğŸ¯ **Key Achievements**
- âœ… **Memory-Efficient Training**: QLoRA fine-tuning of 7B models on <16GB GPU memory
- âœ… **Scientific Data Processing**: Automated genomics paper processing and quality scoring
- âœ… **Production Inference Server**: FastAPI-based REST API with real-time monitoring
- âœ… **Comprehensive Testing**: 82% test coverage with async test support
- âœ… **Complete MLOps Pipeline**: End-to-end workflow from data to deployment

## ğŸŒŸ Features

### ğŸ”¬ **Scientific Data Processing**
- **PubMed Integration**: Automated collection of high-quality genomics papers
- **Quality Scoring**: AI-powered paper quality assessment and filtering
- **Scientific Text Processing**: Citation removal, notation normalization, terminology preservation
- **Benchmark Detection**: Automatic identification of datasets and benchmarks used

### ğŸ§  **Advanced Model Training**
- **QLoRA Fine-tuning**: Memory-efficient parameter-efficient fine-tuning
- **Enhanced Trainer**: Extended training pipeline with gradient checkpointing
- **Multi-GPU Support**: Distributed training with DeepSpeed integration
- **MLflow Tracking**: Comprehensive experiment tracking and model versioning
- **Dynamic Batch Sizing**: Automatic memory optimization during training

### ğŸš€ **Production Inference Server**
- **FastAPI REST API**: Production-ready async inference server
- **Genomics Endpoints**: Specialized endpoints for gene queries and pathway analysis
- **Paper Analysis**: Scientific paper summarization and key findings extraction
- **Performance Monitoring**: Real-time metrics, health checks, and auto-scaling
- **Memory Optimization**: Efficient model loading with quantization support

### ğŸ› ï¸ **MLOps Infrastructure**
- **Configuration Management**: YAML-based configuration system
- **Comprehensive Logging**: Structured logging with performance metrics
- **Testing Framework**: Unit, integration, and async test suites
- **CI/CD Ready**: Pre-commit hooks and automated quality checks
- **Docker Support**: Containerized deployment configurations

## ğŸ“ Project Structure

```
scientific-llm-forge/
â”œâ”€â”€ ğŸ“‚ src/                           # Core source code
â”‚   â”œâ”€â”€ ğŸ“Š data/                     # Scientific data processing
â”‚   â”‚   â”œâ”€â”€ scientific_dataset.py    # High-quality paper dataset loader
â”‚   â”‚   â”œâ”€â”€ text_processor.py        # Genomics-specific text preprocessing
â”‚   â”‚   â”œâ”€â”€ pubmed_client.py         # PubMed API integration
â”‚   â”‚   â””â”€â”€ quality_scorer.py        # AI-powered paper quality assessment
â”‚   â”œâ”€â”€ ğŸ§  models/                   # Advanced model training
â”‚   â”‚   â”œâ”€â”€ enhanced_trainer.py      # QLoRA-enhanced training pipeline
â”‚   â”‚   â”œâ”€â”€ model_loader.py          # Memory-efficient model loading
â”‚   â”‚   â”œâ”€â”€ qlora_config.py          # QLoRA configuration management
â”‚   â”‚   â””â”€â”€ checkpoint_manager.py    # Model checkpoint handling
â”‚   â”œâ”€â”€ ğŸš€ serving/                  # Production inference server
â”‚   â”‚   â”œâ”€â”€ inference_server.py      # FastAPI inference server
â”‚   â”‚   â”œâ”€â”€ api.py                   # API endpoint definitions
â”‚   â”‚   â””â”€â”€ deployment.py           # Deployment configurations
â”‚   â””â”€â”€ ğŸ› ï¸ utils/                    # Utility functions
â”‚       â”œâ”€â”€ logger.py                # Structured logging
â”‚       â”œâ”€â”€ config.py                # Configuration management
â”‚       â””â”€â”€ metrics.py               # Performance metrics
â”œâ”€â”€ ğŸ“‹ configs/                      # Configuration files
â”‚   â”œâ”€â”€ training.yaml                # Training configuration
â”‚   â”œâ”€â”€ serving.yaml                 # Serving configuration
â”‚   â””â”€â”€ deepspeed_config.json        # Distributed training config
â”œâ”€â”€ ğŸ§ª tests/                        # Comprehensive test suite
â”‚   â”œâ”€â”€ test_inference_server.py     # FastAPI server tests (21 tests)
â”‚   â”œâ”€â”€ test_enhanced_trainer.py     # Training pipeline tests
â”‚   â””â”€â”€ test_*.py                    # Component-specific tests
â”œâ”€â”€ ğŸ“š examples/                     # Usage examples and demos
â”‚   â”œâ”€â”€ inference_server_example.py  # FastAPI server demo
â”‚   â”œâ”€â”€ enhanced_trainer_example.py  # Training pipeline demo
â”‚   â””â”€â”€ *.py                         # Component examples
â”œâ”€â”€ ğŸ“œ scripts/                      # Automation scripts
â”œâ”€â”€ ğŸ“„ .kiro/specs/                  # Feature specifications
â””â”€â”€ ğŸ“¦ requirements.txt              # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** (3.12 recommended)
- **CUDA-capable GPU** (optional, for training)
- **16GB+ RAM** (8GB minimum)
- **Git**

### âš¡ One-Command Setup

```bash
# Clone and setup everything
git clone https://github.com/scientificllmforge/scientific-llm-forge.git
cd scientific-llm-forge
python scripts/setup_dev.py && source activate_dev.sh
```

This automatically:
- âœ… Creates Python virtual environment
- âœ… Installs all dependencies (PyTorch, FastAPI, Transformers, etc.)
- âœ… Sets up pre-commit hooks for code quality
- âœ… Creates example configurations
- âœ… Initializes logging and monitoring

### ğŸ”¥ Complete Workflow Demo

#### 1. **Start the Inference Server**
```bash
# Start FastAPI server with auto-reload
python -m src.serving.inference_server
# Server starts at: http://localhost:8000
# API docs at: http://localhost:8000/docs
```

#### 2. **Load a Fine-tuned Model**
```bash
# Load model from checkpoint
curl -X POST "http://localhost:8000/api/v1/load-model" \
  -H "Content-Type: application/json" \
  -d '{"model_path": "/path/to/checkpoint", "use_quantization": true}'
```

#### 3. **Query Genomics Information**
```bash
# Ask genomics questions
curl -X POST "http://localhost:8000/api/v1/genomics/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the function of BRCA1?", "query_type": "gene_function"}'
```

#### 4. **Analyze Scientific Papers**
```bash
# Analyze research papers
curl -X POST "http://localhost:8000/api/v1/papers/analyze" \
  -H "Content-Type: application/json" \
  -d '{"title": "CRISPR gene editing study", "abstract": "...", "analysis_type": "summary"}'
```

### ğŸ§  Training Your Own Model

#### **Memory-Efficient QLoRA Fine-tuning**
```bash
# Train 7B model on <16GB GPU
python examples/enhanced_trainer_example.py

# Or use the training script
python scripts/train.py --config configs/training.yaml
```

#### **Process Scientific Data**
```bash
# Load and process genomics papers
python examples/scientific_dataset_example.py

# Collect data from PubMed
python scripts/collect_pubmed_data.py --query "CRISPR genomics" --max_papers 1000
```

### ğŸ§ª Run Tests

```bash
# Run all tests (82% coverage)
pytest tests/ -v

# Run specific component tests
pytest tests/test_inference_server.py -v  # 21 FastAPI tests
pytest tests/test_enhanced_trainer.py -v  # Training pipeline tests
pytest tests/test_scientific_dataset.py -v  # Data processing tests
```

## ğŸ”§ Configuration

### **Training Configuration** (`configs/training.yaml`)

```yaml
training:
  model:
    name: "meta-llama/Llama-2-7b-hf"  # Support for LLaMA-2
    model_type: "llama"
    max_length: 2048
    
  # QLoRA configuration for memory efficiency
  qlora:
    enabled: true
    r: 16                    # LoRA rank
    lora_alpha: 32          # LoRA scaling
    lora_dropout: 0.1       # LoRA dropout
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: "nf4"
      
  # Scientific data processing
  scientific_data:
    data_file: "data/high_quality_papers_demo.json"
    text_fields: ["title", "abstract", "full_text"]
    preprocessing:
      remove_citations: true
      normalize_scientific_notation: true
      handle_special_tokens: true
      
  # MLflow experiment tracking
  mlflow:
    enabled: true
    experiment_name: "genomics-llm-finetuning"
    tracking_uri: "file:./mlruns"
    
  hyperparameters:
    learning_rate: 2e-4
    batch_size: 4           # Optimized for memory
    gradient_accumulation_steps: 4
    num_epochs: 3
    warmup_steps: 100
    weight_decay: 0.01
```

### **Serving Configuration** (`configs/serving.yaml`)

```yaml
serving:
  server:
    host: "0.0.0.0"
    port: 8000
    workers: 1
    reload: false
    
  model:
    checkpoint_path: "/path/to/fine-tuned/model"
    use_quantization: true
    device: "auto"          # Auto-detect GPU/CPU
    
  inference:
    max_length: 512
    temperature: 0.7
    top_p: 0.9
    batch_size: 8
    
  monitoring:
    enable_metrics: true
    log_requests: true
    health_check_interval: 30
```

### **DeepSpeed Configuration** (`configs/deepspeed_config.json`)

```json
{
  "train_batch_size": 16,
  "gradient_accumulation_steps": 4,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-4,
      "weight_decay": 0.01
    }
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  }
}
```

## ğŸ—ï¸ Architecture & Components

### **FastAPI Inference Server** (`src/serving/inference_server.py`)

```python
# Production-ready async inference server
from src.serving.inference_server import app, run_server

# Start server
run_server(host="0.0.0.0", port=8000, workers=4)
```

**Key Endpoints:**
- `POST /api/v1/generate` - General text generation
- `POST /api/v1/genomics/query` - Genomics-specific queries  
- `POST /api/v1/papers/analyze` - Scientific paper analysis
- `GET /api/v1/health` - Health checks and monitoring
- `GET /api/v1/metrics` - Performance metrics
- `GET /docs` - Interactive API documentation

### **Enhanced Training Pipeline** (`src/models/enhanced_trainer.py`)

```python
# Memory-efficient QLoRA training
from src.models.enhanced_trainer import EnhancedModelTrainer
from src.models.qlora_config import QLoRAConfig

# Configure QLoRA for 7B model on <16GB GPU
config = QLoRAConfig(
    r=16, lora_alpha=32, 
    load_in_4bit=True,
    target_modules=["q_proj", "v_proj"]
)

trainer = EnhancedModelTrainer(config)
trainer.train()  # Memory-efficient training
```

### **Scientific Data Processing** (`src/data/`)

```python
# Load and process genomics papers
from src.data.scientific_dataset import ScientificDataModule
from src.data.text_processor import ScientificTextProcessor

# Load high-quality papers
data_module = ScientificDataModule()
papers = data_module.load_high_quality_papers("data/papers.json")

# Process scientific text
processor = ScientificTextProcessor()
processed_text = processor.preprocess_scientific_text(paper_text)
```

## ğŸ§ª Testing & Quality Assurance

### **Comprehensive Test Suite**
- âœ… **21 FastAPI server tests** (82% coverage)
- âœ… **Async test support** with pytest-asyncio
- âœ… **Mock-based testing** for model interactions
- âœ… **Integration tests** for end-to-end workflows
- âœ… **Performance benchmarking** tests

### **Code Quality Tools**
- **Black**: Automatic code formatting
- **isort**: Import organization
- **flake8**: Code linting and style checks
- **mypy**: Static type checking
- **pre-commit**: Automated quality checks

### **Running Tests**

```bash
# Full test suite with coverage
pytest tests/ --cov=src --cov-report=html

# Specific component tests
pytest tests/test_inference_server.py -v    # FastAPI server (21 tests)
pytest tests/test_enhanced_trainer.py -v    # Training pipeline
pytest tests/test_scientific_dataset.py -v  # Data processing
pytest tests/test_qlora_config.py -v        # QLoRA configuration

# Performance and integration tests
pytest tests/ -m "not slow"  # Skip slow tests
pytest tests/ -m "integration"  # Run integration tests only
```

## ğŸ“Š Performance & Benchmarks

### **Memory Efficiency**
- âœ… **7B Parameter Models**: Fine-tune on <16GB GPU memory
- âœ… **QLoRA Optimization**: 4-bit quantization with LoRA adapters
- âœ… **Dynamic Batch Sizing**: Automatic memory optimization
- âœ… **Gradient Checkpointing**: Reduced memory footprint during training

### **Inference Performance**
- âš¡ **Async Processing**: FastAPI async request handling
- âš¡ **Batch Support**: Multiple queries processed simultaneously
- âš¡ **Memory Monitoring**: Real-time GPU memory tracking
- âš¡ **Auto-scaling**: Dynamic resource allocation based on load

### **Scientific Accuracy**
- ğŸ¯ **Genomics Specialization**: Domain-specific query processing
- ğŸ¯ **Citation Handling**: Proper scientific text preprocessing
- ğŸ¯ **Benchmark Detection**: Automatic dataset identification
- ğŸ¯ **Quality Scoring**: AI-powered paper quality assessment

## ğŸš€ Production Deployment

### **Docker Deployment**

```dockerfile
# Dockerfile example
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY configs/ ./configs/

EXPOSE 8000
CMD ["python", "-m", "src.serving.inference_server"]
```

### **Kubernetes Deployment**

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scientific-llm-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: scientific-llm
  template:
    metadata:
      labels:
        app: scientific-llm
    spec:
      containers:
      - name: inference-server
        image: scientific-llm-forge:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            nvidia.com/gpu: 1
```

### **Monitoring & Observability**

```bash
# Health monitoring
curl http://localhost:8000/api/v1/health

# Performance metrics
curl http://localhost:8000/api/v1/metrics

# MLflow experiment tracking
mlflow ui --backend-store-uri ./mlruns
```

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### **Development Workflow**
1. **Fork** the repository
2. **Create** feature branch: `git checkout -b feature/amazing-feature`
3. **Implement** your changes with tests
4. **Run** quality checks: `pytest tests/ && pre-commit run --all-files`
5. **Submit** pull request with detailed description

### **Contribution Areas**
- ğŸ”¬ **Scientific Domain Expertise**: Add new scientific domains beyond genomics
- ğŸ§  **Model Architectures**: Support for new LLM architectures
- ğŸ“Š **Data Processing**: Enhanced scientific text processing techniques
- ğŸš€ **Deployment**: Kubernetes, cloud deployment configurations
- ğŸ§ª **Testing**: Additional test coverage and benchmarks

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Community

- ğŸ“§ **Issues**: [GitHub Issues](https://github.com/scientificllmforge/scientific-llm-forge/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/scientificllmforge/scientific-llm-forge/discussions)
- ğŸ“š **Documentation**: [Full Documentation](https://scientificllmforge.readthedocs.io)
- ğŸ¦ **Updates**: Follow [@ScientificLLM](https://twitter.com/ScientificLLM)

## ğŸ—ºï¸ Roadmap

### **Completed âœ…**
- âœ… **FastAPI Inference Server** - Production-ready REST API
- âœ… **QLoRA Fine-tuning** - Memory-efficient training pipeline
- âœ… **Scientific Data Processing** - Genomics paper processing
- âœ… **Comprehensive Testing** - 82% test coverage
- âœ… **MLflow Integration** - Experiment tracking
- âœ… **Performance Monitoring** - Real-time metrics

### **In Progress ğŸš§**
- ğŸš§ **Distributed Training** - Multi-GPU DeepSpeed integration
- ğŸš§ **Model Versioning** - Advanced checkpoint management
- ğŸš§ **Web Dashboard** - Training and monitoring UI

### **Planned ğŸ“‹**
- ğŸ“‹ **Multi-Domain Support** - Chemistry, biology, physics datasets
- ğŸ“‹ **Advanced Augmentation** - Scientific text augmentation techniques
- ğŸ“‹ **Cloud Deployment** - AWS, GCP, Azure deployment guides
- ğŸ“‹ **Kubernetes Operators** - Native K8s integration
- ğŸ“‹ **Model Hub Integration** - HuggingFace Hub publishing
- ğŸ“‹ **Real-time Streaming** - WebSocket inference endpoints

---

<div align="center">

**ğŸ§¬ ScientificLLM-Forge: Advancing Scientific Discovery Through AI ğŸš€**

*Built with â¤ï¸ for the scientific research community*

[â­ Star us on GitHub](https://github.com/scientificllmforge/scientific-llm-forge) | [ğŸ“– Read the Docs](https://scientificllmforge.readthedocs.io) | [ğŸ¤ Contribute](CONTRIBUTING.md)

</div>
